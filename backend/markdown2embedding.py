import os
import re
import tiktoken
from dotenv import load_dotenv
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.docstore.document import Document

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

# multi-language suported by text-embedding-ada-002 
tokenizer = tiktoken.encoding_for_model("text-embedding-ada-002")

def split_by_tables_combined(text):
    """
    å°†æ–‡æœ¬æŒ‰è¡¨æ ¼æ ‡è®°æ‹†åˆ†ï¼Œç¡®ä¿ä»¥ __TABLE\d+__ å¼€å¤´çš„éƒ¨åˆ†åŠå…¶åç»­å†…å®¹ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡è®°æˆ–ç»“å°¾ï¼‰
    ä¿æŒåœ¨åŒä¸€ä¸ª chunk ä¸­ã€‚
    ä¾‹å¦‚ï¼š
      "Some text... __TABLE9__:The table presents cutting speeds... More text..."
    è¿”å›ï¼š
      ["Some text...", "__TABLE9__:The table presents cutting speeds...", " More text..."]
    """
    pattern = r"(__TABLE\d+__[:\s]*)"
    parts = re.split(pattern, text)
    chunks = []
    i = 0
    if parts and not re.match(pattern, parts[0]):
        chunks.append(parts[0])
        i = 1
    while i < len(parts):
        if i + 1 < len(parts):
            combined = parts[i] + parts[i+1]
            chunks.append(combined)
            i += 2
        else:
            chunks.append(parts[i])
            i += 1
    return chunks

def smart_chunking(text, max_tokens=1000):
    """
    æ™ºèƒ½ Chunkingï¼š
      - å…ˆä½¿ç”¨ split_by_tables_combined() ä¿è¯è¡¨æ ¼å—å®Œæ•´ï¼›
      - å¯¹éè¡¨æ ¼éƒ¨åˆ†ï¼Œå¦‚æœ token æ•°è¶…è¿‡ max_tokensï¼Œåˆ™è¿›ä¸€æ­¥æ‹†åˆ†æˆå¤šä¸ªå­å—ã€‚
    """
    initial_chunks = split_by_tables_combined(text)
    final_chunks = []
    for chunk in initial_chunks:
        # å¦‚æœ chunk æ˜¯ä»¥ __TABLE\d+__ å¼€å¤´ï¼ˆè¡¨æ ¼å—ï¼‰ï¼Œç›´æ¥ä¿ç•™
        if re.match(r"^__TABLE\d+__", chunk):
            final_chunks.append(chunk)
        else:
            tokens = tokenizer.encode(chunk)
            if len(tokens) <= max_tokens:
                final_chunks.append(chunk)
            else:
                # æŒ‰ max_tokens æ‹†åˆ†ä¸ºå¤šä¸ªå­å—
                for i in range(0, len(tokens), max_tokens):
                    sub_tokens = tokens[i:i+max_tokens]
                    sub_chunk = tokenizer.decode(sub_tokens)
                    final_chunks.append(sub_chunk)
    return final_chunks

def create_vector_DB(file_path):
    # ä»æ–‡ä»¶è·¯å¾„ä¸­æå–æ–‡ä»¶å
    filename = os.path.splitext(os.path.basename(file_path))[0]
    
    # æ„å»ºæŒä¹…åŒ–ç›®å½•å’Œé›†åˆåç§°
    persist_directory = os.path.join("backend\\VectorDBs", filename)
    collection_name = f"rag-{filename}"

    # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(persist_directory):
        print(f"ğŸ”¹ Vector database already exists at {persist_directory}, skipping creation.")
        return

    with open(file_path, "r", encoding="utf-8") as f:
        text_content = f.read()
        
    chunks = smart_chunking(text_content)
    print(f"ğŸ”¹ Got {len(chunks)} chunks without breaking a table.")

    # å°†å­—ç¬¦ä¸² chunk è½¬æ¢æˆ Document å¯¹è±¡
    docs = [Document(page_content=chunk) for chunk in chunks]

    # åˆ›å»ºå‘é‡æ•°æ®åº“ï¼Œè‡ªåŠ¨è®¡ç®— embeddings å¹¶æŒä¹…åŒ–åˆ°æŒ‡å®šç›®å½•
    vectorstore = Chroma.from_documents(
        documents=docs,
        embedding=OpenAIEmbeddings(openai_api_key=openai_api_key),
        persist_directory=persist_directory,
        collection_name=collection_name,
    )
    vectorstore.persist()
    print(f"ğŸ”¹ Vector database has been successfully persisted, saved to {persist_directory}.")

washed_doc_path = ["backend\\washed_documents\\Summurized_Diametal_Turning.md",]

for file_path in washed_doc_path:
    create_vector_DB(file_path)


